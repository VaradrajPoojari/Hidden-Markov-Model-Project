{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f33d3b32",
   "metadata": {},
   "source": [
    "# Hidden Markov Model Project\n",
    "\n",
    "- Build a Hidden Markov Model\n",
    "- Use Hard EM to do semi-supervised part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5a15884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/qichao/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "import scipy\n",
    "from collections import defaultdict,Counter\n",
    "from random import shuffle,seed,choice,random\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fc0866",
   "metadata": {},
   "source": [
    "### Part 1: HMM Initialization and training\n",
    "\n",
    "We initialize an HMM model in the function `HMM.__init__()`, which takes two arguments a vocabulary `emissions` and a state set `states`. The `HMM` class contains the members\n",
    "\n",
    "* `self.emissions`, the list of word types, and\n",
    "* `self.states`, the list of possible POS tags.\n",
    "* `self.w2i` and `self.i2w`, dictionaries for converting between word tokens like `\"dog\"` and index numbers like 134.\n",
    "* `self.s2i` and `self.i2s`, dictionaries for converting between states like `\"NOUN\"` and index numbers like 12.\n",
    "\n",
    "We initialize three member variables:\n",
    "\n",
    "* `self.init_prob`, an `np.array` of initial state probabilities of shape `1 x sixe_of_state_set`.\n",
    "* `self.emission_prob`, an `np.array` of emission probabilities of shape `size_of_state_set x size_of_vocabulary`. \n",
    "* `self.transition_prob` an `np.array` of transition probabilities of shape `size_of_state_set x size_of_state_set`.\n",
    "\n",
    "All array values are initialized to 0.\n",
    "\n",
    "\n",
    "\n",
    "We fully supervised train the HMM model in the function `HMM.train()`. The function takes a training set `data`, which is a list of tagged sentences, e.g.:\n",
    "```\n",
    "[[(\"the\", \"DET\"),(\"dog\", \"NOUN\"),(\"barks\",\"VERB\")], [(\"the\",\"DET\"),(\"dog\",\"NOUN\")]]\n",
    "```\n",
    "\n",
    "We then convert words and POS tags in `data` into index numbers using the function `HMM.data2i()`. The output will look something like this: \n",
    "\n",
    "```\n",
    "[[(101, 10), (1000, 5), (5, 2)], [(101, 10), (1000, 5)]]\n",
    "```\n",
    "\n",
    "The left element of each pair is an index number corresponding to a word type in the vocabulary and the right element is an index number of a state (i.e. POS tag in our case).\n",
    "\n",
    "We start the actual training by storing **counts** of emissions, transitions and initial states in `self.emission_prob`, `self.transition_prob` and `self.initial_prob` (these are transition probabilities $P(s|{\\rm START})$), respectively. For example, if the word number `101` is emitted twice in the state `10`, then we want `self.emission_prob[10][101] == 2`. Similarly, if we transition from state `10` to state `5` twice in `data`, we want `self.transition_prob[10][5] == 2`.\n",
    "\n",
    "Then **apply add-one smoothing** to all counts, and normalize probabilities according to:\n",
    "\n",
    "$$\\large P_{initial}(s) = \\frac{{\\rm count}_{initial}(s)}{\\sum_{t} {\\rm count}_{initial}(t)}$$\n",
    "\n",
    "$$\\large P_{emission}(w | s) = \\frac{{\\rm count}(w,s)}{{\\rm count}(s)}$$\n",
    "\n",
    "$$\\large P_{transition}(t|s) = \\frac{{\\rm count}(s,t)}{{\\rm count}(s)}$$\n",
    "\n",
    "Finally, convert all probabilities to log-probabilities using $p \\mapsto \\log_2 p$ (note that the base of the logarithm is 2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67c05434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# Symbol used to replace unknown tokens in the input \n",
    "UNK=\"<UNK>\"\n",
    "\n",
    "class HMM:\n",
    "    def __init__(self, emissions, states):\n",
    "            # Vocabulary and tag set.\n",
    "            self.emissions = deepcopy(emissions + [UNK])\n",
    "            self.states = deepcopy(states)\n",
    "\n",
    "            # Use these to convert between strings and ID numbers\n",
    "            self.w2i = {w:i for i, w in enumerate(self.emissions)}\n",
    "            self.i2w = self.emissions\n",
    "            self.s2i = {s:i for i,s in enumerate(self.states)}\n",
    "            self.i2s = self.states\n",
    "\n",
    "            size_of_state_set = len(self.states)\n",
    "            size_of_vocabulary = len(self.emissions)\n",
    "\n",
    "            self.init_prob = np.zeros((1, size_of_state_set))\n",
    "            self.emission_prob = np.zeros((size_of_state_set, size_of_vocabulary))\n",
    "            self.transition_prob = np.zeros((size_of_state_set, size_of_state_set))\n",
    "\n",
    "    def data2i(self, data):\n",
    "        \"\"\" Encode emissions and states into index numbers.\n",
    "            ex is either a sequence of words or a sequence\n",
    "            of word-state pairs.\n",
    "        \"\"\"\n",
    "        idx_data = []\n",
    "        if type(data[0][0]) == type(\"\"):\n",
    "            for ex in data:\n",
    "                idx_data.append([])\n",
    "                for w in ex:\n",
    "                    w = w if w in self.w2i else UNK\n",
    "                    idx_data[-1].append(self.w2i[w])\n",
    "        else:\n",
    "            for ex in data:\n",
    "                idx_data.append([])\n",
    "                for w,s in ex:\n",
    "                    w = w if w in self.w2i else UNK\n",
    "                    idx_data[-1].append((self.w2i[w], self.s2i[s]))\n",
    "        return idx_data\n",
    "\n",
    "    def train(self, data):\n",
    "        self.init_prob[:] = 0\n",
    "        self.emission_prob[:] = 0\n",
    "        self.transition_prob[:] = 0\n",
    "\n",
    "        for sentence in self.data2i(data):\n",
    "            flag = -1\n",
    "            for word, state in sentence:\n",
    "                self.emission_prob[state][word] += 1\n",
    "                if(flag != -1):\n",
    "                    self.transition_prob[flag][state] +=1\n",
    "                    flag = state\n",
    "                if(flag == -1):\n",
    "                    self.init_prob[0][state] += 1\n",
    "                    flag = state\n",
    "\n",
    "        self.init_prob = self.init_prob + 1   \n",
    "        self.init_prob = np.log2((self.init_prob)/(np.sum(self.init_prob)))\n",
    "        self.emission_prob= self.emission_prob + 1\n",
    "        self.transition_prob= self.transition_prob + 1\n",
    "        for i in range(0,len(self.emission_prob)):\n",
    "            self.emission_prob[i]= np.log2((self.emission_prob[i])/(np.sum(self.emission_prob[i])))\n",
    "        for i in range(0,len(self.transition_prob)):\n",
    "            self.transition_prob[i]= np.log2((self.transition_prob[i])/(np.sum(self.transition_prob[i])))\n",
    "\n",
    "\n",
    "    def greedy_decode(self, ex):\n",
    "        \"\"\" Greedy (or beam 1 decoding). \"\"\"\n",
    "        ex = self.data2i([ex])[0]\n",
    "        state_distr = np.array(self.init_prob)\n",
    "        output = []\n",
    "        log_prob = 0\n",
    "        for w in ex:\n",
    "            state_distr += self.emission_prob[:, w].reshape(1,-1)\n",
    "            output.append(state_distr.argmax())\n",
    "            log_prob = state_distr.max()\n",
    "            state_distr = log_prob + self.transition_prob[[output[-1]], :]\n",
    "        return [(self.i2w[w], self.i2s[s]) for w, s in zip(ex, output)], log_prob\n",
    "\n",
    "    def extract_output(self, trellis, back_pointers, ex):\n",
    "        log_prob = trellis[:,-1].max()\n",
    "        output = [trellis[:,-1].argmax()]\n",
    "        while len(output) < len(ex):\n",
    "            output.append(back_pointers[output[-1], len(ex) - len(output)])\n",
    "        output = output[::-1]\n",
    "\n",
    "        return [(self.i2w[w], self.i2s[s]) for w, s in zip(ex, output)], log_prob\n",
    "\n",
    "    def viterbi_decode(self, ex):\n",
    "        \"\"\" Viterbi decoding using loops. \"\"\"\n",
    "        ex = self.data2i([ex])[0]\n",
    "        trellis = np.full((len(self.states), len(ex)), (-float(\"inf\")))\n",
    "        back_pointers = np.full((len(self.states), len(ex)), -1)\n",
    "        trellis[:,0] = self.init_prob[0,:] + self.emission_prob[:, ex[0]]\n",
    "\n",
    "        for i in range(1,len(ex)):\n",
    "            for j in range(len(self.states)):\n",
    "                x = []\n",
    "                for k in range(len(self.states)):\n",
    "                    x.append(trellis[k,i-1] + self.transition_prob[k,j] + self.emission_prob[j,ex[i]])\n",
    "                x = np.array(x)\n",
    "                trellis[j,i] = np.max(x)\n",
    "                back_pointers[j,i] = np.argmax(x)\n",
    "        return self.extract_output(trellis, back_pointers, ex)\n",
    "\n",
    "    def fast_viterbi_decode(self, ex):\n",
    "        \"\"\" Vectorized Viterbi decoding. \"\"\"\n",
    "        ex = self.data2i([ex])[0]\n",
    "        trellis = np.full((len(self.states), len(ex)), (-float(\"inf\")))\n",
    "        back_pointers = np.full((len(self.states), len(ex)), -1)\n",
    "        trellis[:,0] = self.init_prob[0,:] + self.emission_prob[:, ex[0]]\n",
    "        prob = np.full((len(set(self.states)), len(set(self.states))), -float(\"inf\"))\n",
    "        \n",
    "        for i in range(1, len(ex)):\n",
    "            prob = (trellis[:, i-1] + self.transition_prob.T + self.emission_prob[:, ex[i]].reshape(-1, 1))\n",
    "            trellis[:, i] = np.max(prob, axis = 1)\n",
    "            back_pointers[:, i] = np.argmax(prob, axis = 1)\n",
    "        return (self.extract_output(trellis, back_pointers, ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50780f18",
   "metadata": {},
   "source": [
    "### Part 2: Viterbi decoding\n",
    "\n",
    "In this part, we implement Viterbi decoding in two ways. \n",
    "\n",
    "\n",
    "We implement the Viterbi algorithm using loops (i.e. as a non-vectorized algorithm) in the function `HMM.viterbi_decode()`.\n",
    "\n",
    "The function takes a single argument `ex`, a list representing a sentence, e.g.:\n",
    "```\n",
    "[\"The\", \"dog\", \"sleeps\"]\n",
    "```\n",
    "\n",
    "We start by converting `ex` into a list of index numbers using the function `HMM.data2i()`. Note that the function takes a list of sentences as input instead of a single sentence. \n",
    "\n",
    "We then initialize two `np.array` objects: \n",
    "\n",
    "* `trellis`, which contains the Viterbi probabilities $v_i(s)$ for each state $s$ and position $i$ in the sentence, and \n",
    "* `back_pointers`, which contains back pointers. These identify the optimal tag history.\n",
    "\n",
    "Both of these need to have dimension `len(self.states) x len(ex)` and we initialize all values in `trellis` to negative infinity (`-float(\"inf\")`) and all values in `back_pointers` to `-1`.\n",
    "\n",
    "We then start filling the trellis one row at a time:\n",
    "\n",
    "1. We'll first initialize all elements `trellis[s,0]` to the sum of the initial log-probability of state `s` and the emission log-probability of the first input word `ex[0]` in the given state.\n",
    "2. When filling in the cell for state `s` in position `i+1`, we need to loop over all states for position `i` and find the state $r_{max}$ which maximizes $\\log_2 v_{i}(r) + \\log_2 P_{transition}(r,s) + \\log_2 P_{emission}(w,s)$, where $w$ is the $i+1$th token in `ex`. This is the Viterbi log-probability $v_{i+i}(s)$.\n",
    "3. We also store $r_{max}$ in cell `s,i+1` in `back_pointers`.\n",
    "\n",
    "After we've filled in `trellis` and `back_pointers`, we call the function `self.extract_output()` which will extract the output tag sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fa36c8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We will now implement a vectorized version of Viterbi in the function `HMM.fast_viterbi_decode()`, which again takes a single argument: `ex` representing a sentence. A successfully implemented vectorized Viterbit can be substantially faster than a loop-based approach.\n",
    "\n",
    "We start by converting word tokens in `ex` into index numbers and intialize `trellis` and `back_pointers` as in Assignment 2.1.\n",
    "\n",
    "We then initialize the first row of the trellis to the sum of your initial probability vector and emission probabilities for `ex[0]`.\n",
    "\n",
    "When filling in column `i+1`, we start by computing a `len(self.states) x len(self.states)` matrix `log_probs`, where the element `log_probs[r,s]` represents the log-probability: \n",
    "\n",
    "$\\log_2 v_{i}(r) + \\log_2 P_{transition}(r,s) + \\log_2 P_{emission}(w,s)$\n",
    "\n",
    "where $w$ is the token `ex[i+1]`. \n",
    "\n",
    "After we've computed `log_probs`, we need to find the maximal element in each row and assign it to row `i+1` in `trellis`. These will be our Viterbi log-probabilities `v_{i+1}(s)` in row `i+1`. We also need to store the index of the element in `back_pointers`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a964dc2b",
   "metadata": {},
   "source": [
    "### Part 3: Hard EM\n",
    "\n",
    "In this part we use hard EM to train an HMM in a semi-supervised manner. We'll use the Brown corpus and form a small manually annotated training set which is combined with a large amount of unlabeled data.\n",
    "\n",
    "The `accuracy` function below computes tagging accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14145c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(sys_data, gold_data):\n",
    "    \"\"\" Compute tagging accuracy. \"\"\"\n",
    "    sys_tags = [t for ex in sys_data for w,t in ex]\n",
    "    gold_tags = [t for ex in gold_data for w,t in ex]\n",
    "    return 100 * (np.array(sys_tags) == np.array(gold_tags)).sum()/len(gold_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865fb09b",
   "metadata": {},
   "source": [
    "We start by reading the tagged sentences in the Brown corpus using the tag set `\"universal\"`. We then divide the corpus into a train split `train_set`, containing 80% of the sentences in the corpus, and a test split `test_set` containing the remaining 20%.\n",
    "\n",
    "To avoid over-representation of any domain in the training and test set, we assign sentences into the train and test splits evenly over the entire corpus. For every consecutive 10 sentences in the Brown corpus, assign 8 to `train_set` and the remaining 2 to `test_set`. E.g. if the sentences in the Brown corpus are $s_1 ... s_n$, then the test set will contain sentences $s_9, s_{10}, s_{19}, s_{20}, s_{29}, s_{30}, ...$ and all remaining sentences will end up in `train_set`. \n",
    "\n",
    "We also generate `train_input` and `test_input` which contain untagged versions of the sentences in `train_set` and `test_set`, i.e. simply lists of word tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "92ca6518-e4f1-4cce-a3ed-b5631c265888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "tagged_sents = brown.tagged_sents(tagset='universal')\n",
    "test_set = [[(w.lower(),t) for (w,t) in sent] for i, sent in enumerate(tagged_sents) if i%10==8 or i%10==9]\n",
    "train_set = [[(w.lower(),t) for (w,t) in sent] for i, sent in enumerate(tagged_sents) if i%10!=8 and i%10!=9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "505a0c01-c736-47f1-b289-eccf7cca26fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown.sents()\n",
    "test_input = [[w.lower() for w in sent] for i, sent in enumerate(brown.sents()) if i%10==8 or i%10==9]\n",
    "train_input = [[w.lower() for w in sent] for i, sent in enumerate(brown.sents()) if i%10!=8 and i%10!=9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb73bf",
   "metadata": {},
   "source": [
    "We will now generate a small labeled training set `mini_train_set`. Sample every 5000th sentence from `train_set` into the small labeled training set.\n",
    "\n",
    "We also generate `vocab`, a list of word types occurring in `train_set` and `test_set`, as well as `tags`, a list of unique tags occurring in `mini_train_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4f810682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "49815\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Rate at which we sample examples from our large \n",
    "# training set into our small annotated training set\n",
    "INV_TRAIN_SAMPLING_RATE=5000\n",
    "\n",
    "mini_train_set = [sent for i, sent in enumerate(train_set) if i%INV_TRAIN_SAMPLING_RATE==0]\n",
    "print(len(mini_train_set))\n",
    "\n",
    "vocab = list(set([w for sent in (train_set+test_set) for w,t in sent ]))\n",
    "print(len(vocab))\n",
    "\n",
    "tags = list(set([t for sent in mini_train_set for w,t in sent]))\n",
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a812e21c",
   "metadata": {},
   "source": [
    "We nitialize an `HMM` object `hmm` using the vocabulary and tagset. Train the model on `mini_train_set`.\n",
    "\n",
    "Apply inference to the sentences in `test_input` and print tagging accuracy. Initially, we use `HMM.greedy_decode` . Switch to `HMM.fast_viterbi_decode` when it is ready (you should get accuracy close to 50%).\n",
    "\n",
    "This is out baseline accuracy before we run hard EM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "29c7478a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tagging accuracy of HMM.greedy_decode is : 32 %\n",
      "The tagging accuracy of HMM.fast_viterbi_decode is : 49 %\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "hmm = HMM(vocab, tags)\n",
    "hmm.train(mini_train_set)\n",
    "preds_greedy = []\n",
    "preds_viterbi = []\n",
    "for sent in test_input:\n",
    "    output, _ = hmm.greedy_decode(sent)\n",
    "    output_viterbi, _ = hmm.fast_viterbi_decode(sent)\n",
    "    preds_greedy.append(output)\n",
    "    preds_viterbi.append(output_viterbi)\n",
    "print('The tagging accuracy of HMM.greedy_decode is :', round(accuracy(preds_greedy, test_set)), '%')\n",
    "print('The tagging accuracy of HMM.fast_viterbi_decode is :', round(accuracy(preds_viterbi, test_set)), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8736bc",
   "metadata": {},
   "source": [
    "When we run hard EM, we will use tag perplexity as stopping criterio. We now implement the function `get_perplexity` which takes a list $\\mathcal{D}$ of $N$ tagged sentences $(x_i, y_i)$ and log-probabilities $\\log_2 P(x_i, y_i)$ as input. It then returns average per-token tag perplexity as defined by the following formula:\n",
    "\n",
    "$$\\large {\\rm PP}(\\mathcal{D}) = 2^{- \\frac{1}{N} \\cdot \\sum_{i=1}^N \\frac{\\log_2 P(x_i,y_i)}{|x_i|}}$$\n",
    "\n",
    "where $|x_i|$ is the length of sentence $x_i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4040d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perplexity(data):\n",
    "    # your code here\n",
    "    sum_prob = 0\n",
    "    for sent, log_prob in data:\n",
    "        sum_prob += log_prob/len(sent)\n",
    "    return 2**((-1/len(data))*sum_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6115fb7b",
   "metadata": {},
   "source": [
    "Finally, we will implement hard EM. Here we will alternate between:\n",
    "\n",
    "* the **E-step** where you tag both `train_input` and `test_input` using current HMM parameters, and\n",
    "* the **M-step** where you retrain the HMM using the tagged output from the E-step.\n",
    "\n",
    "We'll use perplexity as stopping criterion. Generally the M-step will always reduce perplexity. When this reduction `delta` is smaller than a threshold `PERPLEXITY_TH` (0.1), we will stop the EM algorithm. Start by initializing two variables `old_perplexity` and `delta` (i.e. the change in perplexity) to infinity (`float(\"inf\")`). Also reinitialize an `HMM` object `hmm` using `vocab` and `tags`, and train it on `mini_train_set`.\n",
    "\n",
    "At every step of the algorithm, you should first tag the entire training set `train_input` and the test set `test_input` using your current parameters for `hmm` (use `HMM.fast_viterbi_decode()` if it's available, or `HMM.greedy_decode()`, otherwise). \n",
    "\n",
    "We then compute `perplexity` on the tagged output. Using your old and new perplexity value, compute an updated value for `delta`. Your should also print the current `perplexity` and `delta`. \n",
    "\n",
    "If `delta` is less than `PERPLEXITY_TH`, you can stop. Otherwise, use the tagger output for `train_input` and `test_input` to retrain `hmm`.\n",
    "\n",
    "The perplexity should continuously drop  when using Viterbi, i.e. the perplexity should always be positive (note that this is not necessarily true for `HMM.greedy_decode()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "19e75fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity 93076.10959004151 Delta inf\n",
      "Perplexity 1134.3628365389332 Delta 91941.74675350258\n",
      "Perplexity 1084.83507012278 Delta 49.5277664161531\n",
      "Perplexity 1064.0816053719457 Delta 20.753464750834382\n",
      "Perplexity 1051.7114318912902 Delta 12.370173480655467\n",
      "Perplexity 1039.9612276911428 Delta 11.750204200147436\n",
      "Perplexity 1031.9109166750316 Delta 8.050311016111209\n",
      "Perplexity 1024.4768390261677 Delta 7.434077648863877\n",
      "Perplexity 1018.5216840529934 Delta 5.955154973174331\n",
      "Perplexity 1013.5455550099454 Delta 4.97612904304799\n",
      "Perplexity 1009.2370892480266 Delta 4.3084657619187965\n",
      "Perplexity 1005.6608111121643 Delta 3.5762781358622533\n",
      "Perplexity 1002.7812124225209 Delta 2.8795986896434442\n",
      "Perplexity 1000.8081805225702 Delta 1.9730318999506835\n",
      "Perplexity 999.2727822984341 Delta 1.5353982241360882\n",
      "Perplexity 998.0668605625979 Delta 1.2059217358362275\n",
      "Perplexity 997.2448098339612 Delta 0.8220507286366683\n",
      "Perplexity 996.6971237081076 Delta 0.5476861258536019\n",
      "Perplexity 996.141249934898 Delta 0.5558737732095551\n",
      "Perplexity 995.6284563391832 Delta 0.5127935957148111\n",
      "Perplexity 995.255886456874 Delta 0.3725698823092216\n",
      "Perplexity 994.9598892082015 Delta 0.29599724867250643\n",
      "Perplexity 994.6555269591032 Delta 0.3043622490982898\n",
      "Perplexity 994.3205655494379 Delta 0.3349614096653113\n",
      "Perplexity 993.7960234889014 Delta 0.5245420605365325\n",
      "Perplexity 992.7905219281772 Delta 1.0055015607241558\n",
      "Perplexity 991.465460347368 Delta 1.3250615808092334\n",
      "Perplexity 990.2152479944978 Delta 1.2502123528702214\n",
      "Perplexity 988.9225493459627 Delta 1.2926986485350653\n",
      "Perplexity 987.5064991146934 Delta 1.4160502312693097\n",
      "Perplexity 985.9385677840958 Delta 1.5679313305976166\n",
      "Perplexity 984.6392197458198 Delta 1.2993480382759799\n",
      "Perplexity 983.69058332763 Delta 0.9486364181898352\n",
      "Perplexity 983.0300906853884 Delta 0.6604926422415929\n",
      "Perplexity 982.4933283867384 Delta 0.5367622986499327\n",
      "Perplexity 982.1309315088048 Delta 0.36239687793363373\n",
      "Perplexity 981.8666682859556 Delta 0.2642632228491948\n",
      "Perplexity 981.6463111431461 Delta 0.22035714280946195\n",
      "Perplexity 981.4167590378802 Delta 0.22955210526595238\n",
      "Perplexity 981.3036424507175 Delta 0.11311658716272177\n",
      "Perplexity 981.2224094239649 Delta 0.08123302675255673\n"
     ]
    }
   ],
   "source": [
    "PERPLEXITY_TH = 0.1\n",
    "\n",
    "hmm = HMM(vocab, tags)\n",
    "hmm.train(mini_train_set)\n",
    "\n",
    "old_perplexity = float(\"inf\")\n",
    "delta = float(\"inf\")\n",
    "\n",
    "while delta > PERPLEXITY_TH:\n",
    "    pred_train = []\n",
    "    pred_test = []\n",
    "    for sent in train_input:\n",
    "        output_train, _ = hmm.viterbi_decode(sent)\n",
    "        pred_train.append((output_train, _))\n",
    "    for sent in test_input:\n",
    "        output_test, _ = hmm.viterbi_decode(sent)\n",
    "        pred_test.append((output_test, _))\n",
    "    \n",
    "    new_perplexity = get_perplexity(pred_test)\n",
    "    delta = old_perplexity - new_perplexity\n",
    "    print('Perplexity', new_perplexity, 'Delta', delta)\n",
    "    \n",
    "    old_perplexity = new_perplexity\n",
    "    new_train_set = [sent for sent,num in (pred_train+pred_test)]\n",
    "    new_tags = list(set([t for sent in new_train_set for w,t in sent]))\n",
    "\n",
    "    hmm = HMM(vocab, new_tags)\n",
    "    hmm.train(new_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7240cf",
   "metadata": {},
   "source": [
    "Finally, we tag `test_input` using your EM-trained `hmm` and evaluate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cf33fbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tagging accuracy of EM-trained hmm is : 57 %\n"
     ]
    }
   ],
   "source": [
    "preds_viterbi = []\n",
    "for sent in test_input:\n",
    "    output_viterbi, _ = hmm.viterbi_decode(sent)\n",
    "    preds_viterbi.append(output_viterbi)\n",
    "print('The tagging accuracy of EM-trained hmm is :', round(accuracy(preds_viterbi, test_set)), '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
